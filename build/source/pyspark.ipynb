{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36e13876",
   "metadata": {},
   "source": [
    "# Spark环境搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e42945",
   "metadata": {},
   "source": [
    "## Spark安装(linux)\n",
    "```bash\n",
    "# 下载\n",
    "wget https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
    "\n",
    "# 解压至 /export/server/\n",
    "mkdir /export/server\n",
    "tar -zxvf spark-3.2.1-bin-hadoop3.2.tgz -C \"/export/server\"\n",
    "\n",
    "# 创建软链接\n",
    "ln -s /export/server/spark-3.2.1-bin-hadoop3.2/ /export/server/spark\n",
    "\n",
    "# 运行bin/pyspark命令\n",
    "cd /export/server/spark/; bin/pyspark => JAVA_HOME is not set\n",
    "\n",
    "# jdk安装:\n",
    "tar -zxvf /home/dev/jdk-8u161-linux-x64.tar.gz -C /export/server\n",
    "ln -s jdk1.8.0_161/ jdk8\n",
    "vi /etc/profile # 添加JAVA_HOME环境变量\n",
    "export JAVA_HOME=/export/server/jdk8\n",
    "export PATH=$PATH:$JAVA_HOME/bin\n",
    "\n",
    "# 再次执行bin/pyspark => env: python3: No such file or directory\n",
    "# Anaconda3安装:\n",
    "wget https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh\n",
    "sh Anaconda3-2022.05-Linux-x86_64.sh\n",
    "# 依次输入 enter => yes => /export/server/anaconda3 => yes 安装完成\n",
    "# 重新登录linux终端看见(base)开头表示安装成功\n",
    "vi /etc/profile # 添加PYSPARK_PYTHON环境变量\n",
    "export PYSPARK_PYTHON=/export/server/anaconda3/bin/python\n",
    "\n",
    "# 再次执行/export/server/spark>bin/pyspark => pyspark启动成功，进入交互页面\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc244b19",
   "metadata": {},
   "source": [
    "## 示例运行\n",
    "1. 通过`pyspark`启动交互式运行\n",
    "   ```python\n",
    "    >>> sc.parallelize([1,2,3,4,5]).map(lambda x: x+1).collect()\n",
    "    [2, 3, 4, 5, 6]                                                                 \n",
    "    >>> \n",
    "    ```\n",
    "1. 检查4040端口\n",
    "    ```bash\n",
    "    netstat -anp|grep 4040\n",
    "    (Not all processes could be identified, non-owned process info\n",
    "    will not be shown, you would have to be root to see it all.)\n",
    "    tcp6       0      0 :::4040                 :::*                    LISTEN      -  \n",
    "    ```\n",
    "    > 每一个Spark程序在运行的时候, 会绑定到Driver所在机器的4040端口上. <br/>\n",
    "\t如果4040端口被占用, 会顺延到4041...,可通过浏览器访问 4040端口\n",
    "\n",
    "1. 通过spark-submit执行python脚本\n",
    "    ```bash\n",
    "\t#官方脚本执行\n",
    "\tbin/spark-submit /export/server/spark/examples/src/main/python/pi.py 10\n",
    "\n",
    "\t#自定义脚本执行\n",
    "\tbin/spark-submit /export/demo/helloworld.py \n",
    "    hello,world!\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d251c8c",
   "metadata": {},
   "source": [
    "## `PySpark`库安装\n",
    "\n",
    "```bash\n",
    "#创建虚拟环境\n",
    "conda create -n pyspark python=3.9 \n",
    "\n",
    "#切换虚拟环境\n",
    "conda activate pyspark \n",
    "\n",
    "# 查看python\n",
    "type python => python is /export/server/anaconda3/envs/pyspark/bin/python\n",
    "\n",
    "# 编辑PYSPARK_PYTHON环境变量对应的值\n",
    "vi /etc/profile\n",
    "PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python\n",
    "\n",
    "# 安装pyspark\n",
    "pip install pyspark -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "\n",
    "# 验证PySpark\n",
    "python\n",
    ">>> import pyspark #不报错则安装成功\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e1001d-8a58-4582-ae3c-b298d94e34fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 本地开发环境搭建\n",
    "> vscode + 远程解释器 + 远程代码\n",
    "1. 密码登录服务器配置: 将windows本地的公钥配置在linux服务器`$HOME/.ssh/authorized_keys`中\n",
    "1. vscode安装`remote development`插件,重启vscode\n",
    "1. 添加远程ssh targets\n",
    "\t1. 点击ssh targets \"+\"\n",
    "\t1. 弹出框输入 ssh <linux username>@<your ip address>回车\n",
    "\t1. 选择 C:\\\\Users\\\\xxx\\\\.ssh\\\\config\n",
    "\t1. 编辑config文件\n",
    "\t```\n",
    "\tHost xxx\n",
    "\tHostName xxx\n",
    "\tUser xxx\n",
    "\tForwardAgent yes\n",
    "\tIdentityFile C:\\\\Users\\\\xxx\\\\.ssh\\\\id_rsa\n",
    "\t```\n",
    "1. vscode安装python插件\n",
    "1. 添加远程python解释器\n",
    "\t1. Ctrl + Shift + p打开命令面板\n",
    "\t1. 输入`Python: Select Interpreter`选择解释器\n",
    "\t1. 首次选择的是`/export/server/anaconda3/envs/pyspark/bin/python`\n",
    "1. 编写helloworld.py代码\n",
    "1. 执行（使用远程解释器)\n",
    "1. 提升缺少package,安装python包`pip install jupyter notebook -i https://pypi.tuna.tsinghua.edu.cn/simple`\n",
    "1. 运行成功"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ec8af3",
   "metadata": {},
   "source": [
    "## spark安装(windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6185be6a",
   "metadata": {},
   "source": [
    "1. jdk安装(1.8)\n",
    "2. 下载`spark-3.2.1-bin-hadoop2.7.zip`,`hadoop-2.7.7.zip`\n",
    "3. 解压至`D:\\export\\server\\`目录下\n",
    "4. 下载`winutils.exec`放置在`hodoop-2.7.7\\bin\\`目录下\n",
    "5. 环境变量配置`SPARK_HOME=D:\\export\\server\\spark-3.2.1-bin-hadoop2.7`,`HADOOP_HOME=D:\\export\\server\\hadoop-2.7.7`\n",
    "6. 修改PATH环境变量配置添加`%SPARK_HOME%\\bin`\n",
    "7. 配置`PYSPARK_PYTHON=C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\python.exe`(否则报错`: org.apache.spark.SparkException: Python worker failed to connect back`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337ed677",
   "metadata": {},
   "source": [
    "spark-submit --conf spark.sql.queryExecutionListeners=\"org.example.framework.LineageListener\" --jars demo-scala-1.0-SNAPSHOT.jar E:\\git\\docs\\bigdata\\samples\\pyspark\\sparksql\\test_lineage.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26e628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark --packages io.delta:delta-core_2.12:2.1.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e816079a",
   "metadata": {},
   "source": [
    "# PySpark版本查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba58af3-0063-433f-a4c5-daa58b62713c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "pyspark.__version__\n",
    "\n",
    "import os\n",
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc2aed-a53f-4ba7-8cbf-5f22f60971f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efa764c-b769-4b96-bfa2-dae778375e2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "df = spark.read.json(f\"{os.environ['SPARK_HOME']}/examples/src/main/resources/people.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d46c20-f336-4a86-8344-ca939df96317",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spark SQL(PySpark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0afe2b3-b53a-45f1-bebd-01ba7fa5e796",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b61c741-1ce6-49ad-9ca8-ebb0be56cad1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generic Load/Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e25bafd-c267-47a1-8317-db2b8957d97a",
   "metadata": {},
   "source": [
    "#### load parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2e4ca8-0f6c-4462-9931-659f6b57e527",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "df = spark.read.load(f\"{os.environ['SPARK_HOME']}/examples/src/main/resources/users.parquet\")\n",
    "df.select(\"name\", \"favorite_color\").write.mode('overwrite').save(\"spark_data/namesAndFavColors.parquet\")\n",
    "df.show()\n",
    "\n",
    "df = spark.read.parquet(f\"{os.environ['SPARK_HOME']}/examples/src/main/resources/users.parquet\")\n",
    "(df.write.format(\"parquet\")\n",
    "    .option(\"parquet.bloom.filter.enabled#favorite_color\", \"true\")\n",
    "    .option(\"parquet.bloom.filter.expected.ndv#favorite_color\", \"1000000\")\n",
    "    .option(\"parquet.enable.dictionary\", \"true\")\n",
    "    .option(\"parquet.page.write-checksum.enabled\", \"false\")\n",
    "    .save(\"spark_data/users_with_options.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945a46ec-f4b1-4339-b0b0-d7342cb96f25",
   "metadata": {},
   "source": [
    "#### load json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e9674c-6a37-4c46-82cb-699c25737400",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.load(f\"{os.environ['SPARK_HOME']}/examples/src/main/resources/people.json\", format=\"json\")\n",
    "df.select(\"name\", \"age\").write.save(\"spark_data/namesAndAges.parquet\", format=\"parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b5c08-a588-4db6-b74d-fdd262cfafbf",
   "metadata": {},
   "source": [
    "#### load csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c0f70-9ca4-4ffc-9feb-4a728a8dcf22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.load(f\"{os.environ['SPARK_HOME']}/examples/src/main/resources/people.csv\",\n",
    "                     format=\"csv\", sep=\";\", inferSchema=\"true\", header=\"true\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ed0fa-31a8-4662-9f6c-eea3affed17b",
   "metadata": {},
   "source": [
    "#### load orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ec3a9-b05a-410c-8140-e40a610a0868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.orc(f\"{os.environ['SPARK_HOME']}/examples/src/main/resources/users.orc\")\n",
    "t=(df.write.mode(\"overwrite\").format(\"orc\")\n",
    "    .option(\"orc.bloom.filter.columns\", \"favorite_color\")\n",
    "    .option(\"orc.dictionary.key.threshold\", \"1.0\")\n",
    "    .option(\"orc.column.encoding.direct\", \"name\")\n",
    "    .save(\"spark_data/users_with_options.orc\"))\n",
    "df.show()\n",
    "display(t)\n",
    "\n",
    "# python () {} []中多行无需写 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba42dcd1-02a0-432e-9d25-2cb6de1fdf56",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Run SQL on files directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef894ee-810f-4f3f-917a-13eabf30774b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.sql(f\"SELECT * FROM parquet.`{os.environ['SPARK_HOME']}/examples/src/main/resources/users.parquet`\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cf8d6e-350b-428f-811c-1e8111efc897",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff06c25-d079-4467-b59a-f251cc19272c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=spark.read.json(f\"{os.environ['SPARK_HOME']}/examples/src/main/resources/people.json\")\n",
    "df.write.mode('overwrite').parquet('spark_data/people.parquet')\n",
    "\n",
    "df=spark.read.parquet('spark_data/people.parquet')\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "df.createOrReplaceTempView('parquetFile')\n",
    "df=spark.sql('select name from parquetFile where age>=13 and age<=19')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e38f737-8f94-43b7-8ae3-1c287a310fed",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Schema Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13836d0f-35be-497a-a51c-da4d652da023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "sc=spark.sparkContext\n",
    "\n",
    "squaresDF=spark.createDataFrame(sc.parallelize(range(1,6)).map(lambda i:Row(single=i,double=i**2)))\n",
    "squaresDF.show()\n",
    "squaresDF.write.mode('overwrite').parquet('spark_data/test_table/key=1')\n",
    "\n",
    "cubesDF=spark.createDataFrame(sc.parallelize(range(6,11)).map(lambda i:Row(single=i,triple=i**3)))\n",
    "cubesDF.show()\n",
    "cubesDF.write.mode('overwrite').parquet('spark_data/test_table/key=2')\n",
    "\n",
    "mergedDF=spark.read.option('mergeSchema', 'true').parquet('spark_data/test_table')\n",
    "mergedDF.printSchema()\n",
    "mergedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f0229d-6f4d-4f3f-aabf-eb37c4ffb5f6",
   "metadata": {},
   "source": [
    "### Json Files\n",
    "- the built-in functions below\n",
    "    - from_json\n",
    "    - to_json\n",
    "    - schema_of_json\n",
    "\n",
    "- data source option\n",
    "    - primitivesAsString: true/false -- Infers all primitive values as a string\n",
    "    - ignoreNullFields: true/false -- Whether to ignore null fields when generating JSON objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f006a7f-f7a5-45b4-ace7-acbbc68b78de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc=spark.sparkContext\n",
    "path=f\"{os.environ['SPARK_HOME']}/examples/src/main/resources/people.json\"\n",
    "df=spark.read.json(path)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "jsonStrings=['{\"name\":\"Yin\",\"address\":{\"city\":\"columbus\",\"state\":\"Ohio\"}}']\n",
    "rdd=sc.parallelize(jsonStrings)\n",
    "df=spark.read.json(rdd)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c8ecc-a59c-47d5-8cba-e9705046ce7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jsonStrings=['{\"name\":\"Yin\",\"address\":{\"city\":\"columbus\",\"state\":\"Ohio\"}, \"number\": 123 }']\n",
    "rdd=sc.parallelize(jsonStrings)\n",
    "\n",
    "df=spark.read.json(rdd) # `number` type is long\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "df=spark.read.option('primitivesAsString', 'true').json(rdd) # `number` type is string\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b9a2da-fae8-4ee6-a33c-a90cea32e096",
   "metadata": {},
   "source": [
    "### CSV Files\n",
    "- built-in functions\n",
    "    - from_csv\n",
    "    - to_csv\n",
    "    - schema_of_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cf21cb-4ae6-40a3-877a-f2ed8f5b4c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc=spark.sparkContext\n",
    "path=f\"{os.environ['SPARK_HOME']}/examples/src/main/resources/people.csv\"\n",
    "df=spark.read.csv(path)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "# Read a csv with delimiter\n",
    "df2=spark.read.option('delimiter', ';').csv(path) # delimiter default is ','\n",
    "df2.printSchema()\n",
    "df2.show()\n",
    "\n",
    "# Read a csv with delimiter and a header\n",
    "df3=spark.read.option('delimiter',';').option('header', True).csv(path)\n",
    "df3=spark.read.options(delimiter=';', header=True).csv(path)\n",
    "df3.printSchema()\n",
    "df3.show()\n",
    "\n",
    "df3.write.mode('overwrite').csv('spark_data/output')\n",
    "\n",
    "# Wrong schema because non-CSV files are read\n",
    "folderPath=f\"{os.environ['SPARK_HOME']}/examples/src/main/resources\"\n",
    "df5=spark.read.csv(folderPath)\n",
    "df5.printSchema()\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c864ff8-6b70-4b91-b2dc-cf0350f716f3",
   "metadata": {},
   "source": [
    "### Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0acc6c-d492-4624-a58d-9d688819c991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc=spark.sparkContext\n",
    "path=f\"{os.environ['SPARK_HOME']}/examples/src/main/resources/people.txt\"\n",
    "df=spark.read.text(path)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "df=spark.read.text(path, lineSep=',')\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "df=spark.read.text(path, wholetext=True)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad6da9-26b7-4a0e-8d1e-ab84ed7dfd00",
   "metadata": {},
   "source": [
    "### JDBC To Other Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ecb64e-3158-4059-8e7f-336fb1ef63f5",
   "metadata": {},
   "source": [
    "## Performance Tuning\n",
    "\n",
    "### Caching Data In Memery\n",
    "- spark.catalog.cacheTable('tableName')/spark.catalog.uncacheTable('tableName')\n",
    "- dataFrame.cache()/dataFrame.unpersist()\n",
    "\n",
    "### Join Strategy Hints for SQL Queries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
